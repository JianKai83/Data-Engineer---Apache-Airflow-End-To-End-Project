version: '3.8'
services:
  initdb:
    image: dataflow:12.8
    command: pipenv run airflow db init
    restart: on-failure
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: [node.labels.airflow == true]
    networks:
        - my_network

  create-user:
    image: dataflow:12.8
    command: pipenv run airflow users create --username admin --firstname ryan --lastname liao --role Admin -p admin --email ckliao820831@gmail.com
    depends_on:
      - initdb
    restart: on-failure
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: [node.labels.airflow == true]
    networks:
        - my_network

  redis:
    image: 'bitnami/redis:5.0'
    ports:
        - 6379:6379
    volumes:
        - 'redis_data:/bitnami/redis/data'
    environment:
        - ALLOW_EMPTY_PASSWORD=yes
    restart: always
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: [node.labels.airflow == true]
    networks:
        - my_network

  webserver:
    image: dataflow:12.8
    hostname: "airflow-webserver"
    command: pipenv run airflow webserver -p 8888
    depends_on:
      - initdb
    restart: always
    ports:
        - 8888:8888
    environment:
      - TZ=Asia/Taipei
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: [node.labels.airflow == true]
    networks:
        - my_network

  flower:
    image: mher/flower:0.9.5
    restart: always
    depends_on:
        - redis
    command: ["flower", "--broker=redis://redis:6379/0", "--port=5555"]
    ports:
        - "5555:5555"
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: [node.labels.airflow == true]
    networks:
        - my_network
  
  scheduler:
    image: dataflow:12.8
    hostname: "airflow-scheduler"
    command: pipenv run airflow scheduler
    depends_on:
      - initdb
    restart: always
    environment:
      - TZ=Asia/Taipei
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints: [node.labels.airflow == true]
    networks:
        - my_network

  worker:
    image: dataflow:12.8
    hostname: "{{.Service.Name}}.{{.Task.Slot}}"
    restart: always
    depends_on:
        - scheduler
    command: pipenv run airflow celery worker
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 3
      placement:
        constraints: [node.labels.worker == true]
    networks:
        - my_network

  crawler_104:
    image: dataflow:12.8
    hostname: "{{.Service.Name}}.{{.Task.Slot}}"
    restart: always
    depends_on:
        - scheduler
    command: pipenv run airflow celery worker -q job_104
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 2
      placement:
        max_replicas_per_node: 1
        constraints: [node.labels.crawler_104 == true]
    networks:
        - my_network

  crawler_1111:
    image: dataflow:12.8
    hostname: "{{.Service.Name}}.{{.Task.Slot}}"
    restart: always
    depends_on:
        - scheduler
    command: pipenv run airflow celery worker -q job_1111
    # swarm 設定
    deploy:
      mode: replicated
      replicas: 2
      placement:
        max_replicas_per_node: 1
        constraints: [node.labels.crawler_1111 == true]
    networks:
        - my_network

networks:
  my_network:
    external: true

volumes:
  redis_data: